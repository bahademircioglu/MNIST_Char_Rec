import numpy as np
from keras.datasets import mnist
from keras import models, layers, activations, optimizers
from keras.utils import to_categorical
import matplotlib.pyplot as plt

# Load the mnist dataset
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

# Select randomly 500 samples (50 samples for each class)
selected_train_images = []
selected_train_labels = []

for i in range(10):
    indices = np.where(train_labels == i)[0][:50] # Get 50 indices for each class
    selected_train_images.extend(train_images[indices])
    selected_train_labels.extend(train_labels[indices])

selected_train_images = np.array(selected_train_images)
selected_train_labels = np.array(selected_train_labels)

# Normalize the images
selected_train_images = selected_train_images.reshape(selected_train_images.shape[0], 28, 28, 1) / 255.0

# Convert labels to one-hot encoding
selected_train_labels = to_categorical(selected_train_labels)

def create_model():
    model = models.Sequential()
    model.add(layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))
    model.add(layers.MaxPooling2D(pool_size=(2, 2)))
    model.add(layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))
    model.add(layers.MaxPooling2D(pool_size=(2, 2)))
    model.add(layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))
    model.add(layers.Flatten())
    model.add(layers.Dense(64, activation='relu'))
    model.add(layers.Dense(10, activation='softmax'))
    return model

# Define the 4-fold cross validation
k = 4
num_val_samples = len(selected_train_images) // k
num_epochs = 5
all_histories = []

for i in range(k):
    print(f'Processing fold #{i}')
    val_data = selected_train_images[i * num_val_samples: (i + 1) * num_val_samples]
    val_labels = selected_train_labels[i * num_val_samples: (i + 1) * num_val_samples]
    partial_train_data = np.concatenate([selected_train_images[:i * num_val_samples],
    selected_train_images[(i + 1) * num_val_samples:]], axis=0)
    partial_train_labels = np.concatenate([selected_train_labels[:i * num_val_samples],
    selected_train_labels[(i + 1) * num_val_samples:]], axis=0)
    model = create_model()
    model.compile(optimizer='rmsprop',
    loss='categorical_crossentropy',
    metrics=['accuracy'])
    history = model.fit(partial_train_data, partial_train_labels,
    epochs=num_epochs, batch_size=64, verbose=1,
    validation_data=(val_data, val_labels))
    all_histories.append(history.history)

# Plot accuracy
for i, history in enumerate(all_histories):
    plt.plot(range(1, num_epochs + 1), history['accuracy'], label=f'Fold {i+1}')
average_val_acc = np.mean([history['accuracy'] for history in all_histories], axis=0)
plt.plot(range(1, num_epochs + 1), average_val_acc, 'k--', label='Average Accuracy')

plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Accuracy over Epochs')
plt.legend()
plt.show()

# Plot loss
for i, history in enumerate(all_histories):
    plt.plot(range(1, num_epochs + 1), history['loss'], label=f'Fold {i+1}')
average_val_loss = np.mean([history['loss'] for history in all_histories], axis=0)

plt.plot(range(1, num_epochs + 1), average_val_loss, 'k--', label='Average Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Loss over Epochs')
plt.legend()
